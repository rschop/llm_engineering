{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ab6d0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39320192",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = (\n",
    "    # \"/home/rschop/llm_engineering/caorijk/caorijk.pdf\"\n",
    "    \"/home/rschop/llm_engineering/caorijk/layout-parser-paper.pdf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b878eb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "pages = []\n",
    "async for page in loader.alazy_load():\n",
    "    pages.append(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "499887cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-22T01:27:10+00:00', 'author': '', 'keywords': '', 'moddate': '2021-06-22T01:27:10+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/home/rschop/llm_engineering/caorijk/layout-parser-paper.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4'}\n",
      "\n",
      "4 Z. Shen et al.\n",
      "Efficient Data Annotation\n",
      "C u s t o m i z e d  M o d e l  T r a i n i n g\n",
      "Model Cust omization\n",
      "DI A Model Hub\n",
      "DI A Pipeline Sharing\n",
      "Community Platform\n",
      "La y out Detection Models\n",
      "Document Images \n",
      "T h e  C o r e  L a y o u t P a r s e r  L i b r a r y\n",
      "OCR Module St or age & VisualizationLa y out Data Structur e\n",
      "Fig. 1: The overall architecture of LayoutParser. For an input document image,\n",
      "the core LayoutParser library provides a set of oﬀ-the-shelf tools for layout\n",
      "detection, OCR, visualization, and storage, backed by a carefully designed layout\n",
      "data structure. LayoutParser also supports high level customization via eﬃcient\n",
      "layout annotation and model training functions. These improve model accuracy\n",
      "on the target samples. The community platform enables the easy sharing of DIA\n",
      "models and whole digitization pipelines to promote reusability and reproducibility.\n",
      "A collection of detailed documentation, tutorials and exemplar projects make\n",
      "LayoutParser easy to learn and use.\n",
      "AllenNLP [8] and transformers [ 34] have provided the community with complete\n",
      "DL-based support for developing and deploying models for general computer\n",
      "vision and natural language processing problems. LayoutParser, on the other\n",
      "hand, specializes speciﬁcally in DIA tasks. LayoutParser is also equipped with a\n",
      "community platform inspired by established model hubs such as Torch Hub [23]\n",
      "and TensorFlow Hub [1]. It enables the sharing of pretrained models as well as\n",
      "full document processing pipelines that are unique to DIA tasks.\n",
      "There have been a variety of document data collections to facilitate the\n",
      "development of DL models. Some examples include PRImA [3](magazine layouts),\n",
      "PubLayNet [38](academic paper layouts), Table Bank [ 18](tables in academic\n",
      "papers), Newspaper Navigator Dataset [ 16, 17](newspaper ﬁgure layouts) and\n",
      "HJDataset [31](historical Japanese document layouts). A spectrum of models\n",
      "trained on these datasets are currently available in the LayoutParser model zoo\n",
      "to support diﬀerent use cases.\n",
      "3 The Core LayoutParser Library\n",
      "At the core of LayoutParser is an oﬀ-the-shelf toolkit that streamlines DL-\n",
      "based document image analysis. Five components support a simple interface\n",
      "with comprehensive functionalities: 1) The layout detection models enable using\n",
      "pre-trained or self-trained DL models for layout detection with just four lines\n",
      "of code. 2) The detected layout information is stored in carefully engineered\n"
     ]
    }
   ],
   "source": [
    "print(f\"{pages[3].metadata}\\n\")\n",
    "print(pages[3].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ba7026a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7debb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found and looks good so far!\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Check the key\n",
    "\n",
    "if not api_key:\n",
    "    print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
    "elif not api_key.startswith(\"sk-proj-\"):\n",
    "    print(\"An API key was found, but it doesn't start sk-proj-; please check you're using the right key - see troubleshooting notebook\")\n",
    "elif api_key.strip() != api_key:\n",
    "    print(\"An API key was found, but it looks like it might have space or tab characters at the start or end - please remove them - see troubleshooting notebook\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "055a6d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1baa466c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 0: LayoutParser: A Uniﬁed Toolkit for Deep\n",
      "Learning Based Document Image Analysis\n",
      "Zejiang Shen1 (\u0000 ), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\n",
      "Lee4, Jacob Carlson3, and Weining Li5\n",
      "1 Allen Institute for AI\n",
      "shannons@allenai.org\n",
      "2 Brown University\n",
      "ruochen zhang@brown.edu\n",
      "3 Harvard Universi\n",
      "\n",
      "Page 13: 14 Z. Shen et al.\n",
      "6 Conclusion\n",
      "LayoutParser provides a comprehensive toolkit for deep learning-based document\n",
      "image analysis. The oﬀ-the-shelf library is easy to install, and can be used to\n",
      "build ﬂexible and accurate pipelines for processing documents with complicated\n",
      "structures. It also supports hi\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vector_store = InMemoryVectorStore.from_documents(pages, OpenAIEmbeddings())\n",
    "docs = vector_store.similarity_search(\"What is LayoutParser?\", k=2)\n",
    "for doc in docs:\n",
    "    print(f'Page {doc.metadata[\"page\"]}: {doc.page_content[:300]}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
